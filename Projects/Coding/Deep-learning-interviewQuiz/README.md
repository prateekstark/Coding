# Deep-learning-interviewQuiz

* Why number of hidden units in a layer are suggested to be in powers of 2?
* Which is worse false positive or false negative?
* Can we use heuristic algorithm to optimize Hyperparameters in deep learning?
* What causes Vanishing Gradient Problem?
* How Vanishing Gradient Problem can be solved?
* Explain the bias-variance trade-off.
* How does regularization reduces overfitting?
* Why​ ​dropout can hurt performance when used right before the last layer?
* What are the roles of stride and padding in a convolutional neural network?
* How is kNN different from kmeans clustering?
* Why are non-linear activations not used before the final layer?
* Is it valid to connect from a Layer 8 output back to a Layer 4 input?
* Why are CNNs used primarily in imaging and not so much other tasks?
* Explain batch gradient descent, stochastic gradient descent and mini-match
* Explain Dropout and Batch Normalization.
* How to find the best hyper parameters?
* Why Batch Normalization helps in faster convergence gradient descent.
* Why do RNNs have a tendency to suffer from exploding/ vanishing gradient?
* How does transfer learning work?
* When should you use padding in cnn?
* Why to prefer ReLU over Linear activation functions?
* What is weight initialization in neural networks?
* What are the benefits of mini-batch gradient descent?
* Why are deep networks better than shallow ones?
* what to do when Neural network training loss/testing loss
stays constant?
* What is data normalization and why do we need it?
* How to reduce the gap between validation and training loss?
* What is the role of filter size?
* How can filter size affect accuracy and computational
efficiency?
* What is the ideal filter size to be chosen?
* Difference between l1 and l2 loss?
* Which is better l1 or l2 loss?
* Can a neural network with single ReLu (non-linearity) act as a
linear classifier?
* What is the difference between model parallelism and data parallelism?
* How to calculate the effective receptive field given filter size, padding and stride?
* What happens when we initialize all weights to zero with ReLu activation?
* What is the relationship between fully connected layer and convolutional layer
* What is dilation in CNN?
* What happens when we decrease the batch size to 1?
* How does relu solve vanishing gradient?
* What are 1x1 convolutions and how do they help?
* What is the role of zero padding?
* What happens when pooling is removed completely?
* What are the advantages of max pooling?
* What are some examples of adaptive learning rate optimiser?
* Why do we need/want the bias term?
* How Recall And True Positive Rate Are Related?
* What is the difference between Transfer Learning and
Fine-tuning?
* How inception v4 is different from vgg16 neural network architecture?
* Why does the LSTM structure deduct better than RNN?
* Explain adaptive Learning Rate Algorithm: AdaGrad, RMSProp, Adam, etc.
* How to avoid morbidity in deep learning, saddle point, gradient explosion, gradient dispersion?

to add more :

    Clone the repository
    Add questions
    Commit it
    Push it
    Give a PR

